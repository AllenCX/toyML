{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": false,
    "scrolled": true
   },
   "outputs": [
    {
     "ename": "ImportError",
     "evalue": "No module named xgboost",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mImportError\u001b[0m                               Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-1-06b663497127>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m()\u001b[0m\n\u001b[0;32m      5\u001b[0m \u001b[1;32mimport\u001b[0m \u001b[0mmatplotlib\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mpyplot\u001b[0m \u001b[1;32mas\u001b[0m \u001b[0mplt\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      6\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m----> 7\u001b[1;33m \u001b[1;32mimport\u001b[0m \u001b[0mxgboost\u001b[0m \u001b[1;32mas\u001b[0m \u001b[0mxgb\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m      8\u001b[0m \u001b[1;32mfrom\u001b[0m \u001b[0msklearn\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mensemble\u001b[0m \u001b[1;32mimport\u001b[0m \u001b[0mRandomForestClassifier\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      9\u001b[0m \u001b[1;32mfrom\u001b[0m \u001b[0msklearn\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mlinear_model\u001b[0m \u001b[1;32mimport\u001b[0m \u001b[0mLinearRegression\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mImportError\u001b[0m: No module named xgboost"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "\n",
    "import seaborn as sns\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "import xgboost as xgb\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "from sklearn.linear_model import LinearRegression\n",
    "from sklearn.metrics import log_loss, accuracy_score\n",
    "from sklearn.cross_validation import cross_val_score, train_test_split\n",
    "from sklearn import cross_validation, metrics\n",
    "from sklearn.grid_search import GridSearchCV \n",
    "\n",
    "%matplotlib inline\n",
    "%load_ext autoreload\n",
    "%autoreload 2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "data = pd.read_csv('data.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "#to caculate the sum of different action type such as jump shot or running jump shot\n",
    "total_actions = dict(data.action_type.value_counts())\n",
    "\n",
    "threshold = 10\n",
    "data['type'] = data.apply(lambda row: row['action_type'] if total_actions[row['action_type']] >= threshold \\\n",
    "                          else row['combined_shot_type'], axis = 1)\n",
    "\n",
    "data['time_remaining'] = data.apply(lambda row: row['minutes_remaining']*60 + row['seconds_remaining'], axis = 1)\n",
    "\n",
    "# TODO: tune this parameter\n",
    "threshold = 3\n",
    "# TODO: find out why he cant hit @ 14 secs to go\n",
    "anomaly = 14\n",
    "\n",
    "data['last moment'] = data.apply(lambda row: row['time_remaining'] <= threshold or row['time_remaining'] == anomaly, \n",
    "                                 axis = 1)\n",
    "data['shot_distance'] = data.apply(lambda row: 28 if row['shot_distance']>28 else row['shot_distance'], axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "def get_acc(data, field):\n",
    "    ct = pd.crosstab(data.shot_made_flag, data[field]).apply(lambda x: x / x.sum(), axis=0)\n",
    "    x, y = ct.columns, ct.values[1, :]\n",
    "    plt.figure(figsize=(7, 5))\n",
    "    plt.plot(x, y)\n",
    "    plt.xlabel(field)\n",
    "    plt.ylabel('% shots made')\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def sort_encode(data, field):\n",
    "    ct = pd.crosstab(data.shot_made_flag, data[field]).apply(lambda x: x / x.sum(), axis=0)\n",
    "    temp = list(zip(ct.values[1, :], ct.columns))\n",
    "    temp.sort()\n",
    "    new_map = {}\n",
    "    for index, (acc, old_number) in enumerate(temp):\n",
    "        new_map[old_number] = index\n",
    "    new_field = field + '_sort_enumerated'\n",
    "    data[new_field] = data[field].map(new_map)\n",
    "    get_acc(data, new_field)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "data.drop([\n",
    "        'action_type',\n",
    "        'combined_shot_type',\n",
    "        'game_event_id',\n",
    "        'game_id',\n",
    "        'lat',\n",
    "        'lon',\n",
    "        'minutes_remaining',\n",
    "        'seconds_remaining',\n",
    "        'time_remaining',\n",
    "        'team_id',\n",
    "        'team_name',\n",
    "        'matchup',\n",
    "        'game_date',\n",
    "        'shot_type',\n",
    "        'playoffs',\n",
    "        'season',\n",
    "        # TODO: find out whether these two features matter or not\n",
    "        'loc_x',\n",
    "        'loc_y',\n",
    "    ], axis=1, inplace=True)\n",
    "\n",
    "dummies = [\n",
    "    'period',\n",
    "    'type',\n",
    "    'shot_zone_area',\n",
    "    'shot_zone_basic',\n",
    "    'shot_zone_range',\n",
    "    'opponent', #TODO: check if it's needed\n",
    "]\n",
    "dummie_counter = {}\n",
    "for dummy in dummies:\n",
    "    dummie_counter[dummy] = len(data[dummy].unique())\n",
    "data = pd.get_dummies(data, columns=dummies)\n",
    "data.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# split into train and test\n",
    "train = data[~data['shot_made_flag'].isnull()]\n",
    "test = data[data['shot_made_flag'].isnull()]\n",
    "print('train size: ' + str(len(train)))\n",
    "print('test size:  ' + str(len(test)))\n",
    "\n",
    "# prepare data for estimators\n",
    "target = 'shot_made_flag'\n",
    "features = data.columns.tolist()\n",
    "features.remove(target)\n",
    "features.remove('shot_id')\n",
    "X_test = test[features]\n",
    "X_train = train[features]\n",
    "y_train = train[[target]]['shot_made_flag'].values\n",
    "print(X_train.shape)\n",
    "print(y_train.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def modelfit(alg, dtrain, predictors, useTrainCV=True, cv_folds=5, early_stopping_rounds=50):    \n",
    "    if useTrainCV:\n",
    "        xgb_param = alg.get_xgb_params()\n",
    "        xgtrain = xgb.DMatrix(dtrain[predictors].values, label=dtrain[target].values)\n",
    "        cvresult = xgb.cv(xgb_param, xgtrain, num_boost_round=alg.get_params()['n_estimators'], nfold=cv_folds,\n",
    "                          early_stopping_rounds=early_stopping_rounds, metrics=['logloss'])\n",
    "        alg.set_params(n_estimators=cvresult.shape[0])\n",
    "    \n",
    "    # Test params\n",
    "    X_train, X_valid, y_train, y_valid = train_test_split(dtrain[predictors], dtrain[target], test_size=0.2)\n",
    "    alg.fit(X_train, y_train, eval_metric='logloss')\n",
    "    y_pred = alg.predict_proba(X_valid)[:,1]\n",
    "    \n",
    "    result = log_loss(y_valid, y_pred)\n",
    "    print(result)\n",
    "    \n",
    "    feat_imp = pd.Series(alg.booster().get_fscore()).sort_values(ascending=False)\n",
    "    plt.figure(figsize=(20,10))\n",
    "    feat_imp.plot(kind='bar', title='Feature Importances')\n",
    "    plt.ylabel('Feature Importance Score')\n",
    "    \n",
    "    return result"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "params = {\n",
    "    'base_score': 0.5, \n",
    "    'colsample_bylevel': 1,\n",
    "    'colsample_bytree': 0.8,\n",
    "    'learning_rate': 0.035,\n",
    "    'max_depth': 10,\n",
    "    'min_child_weight': 1,\n",
    "    'n_estimators': 200,\n",
    "    'nthread': 8,\n",
    "    'objective': 'binary:logistic',\n",
    "    'seed': 2,\n",
    "    'subsample': 0.8\n",
    "}\n",
    "\n",
    "clf = xgb.XGBClassifier()\n",
    "clf.set_params(**params)\n",
    "modelfit(clf,train,predictors,useTrainCV=True, cv_folds=5, early_stopping_rounds=50)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "clf = xgb.XGBClassifier()\n",
    "clf.set_params(**params)\n",
    "\n",
    "\n",
    "predictors = [x for x in X_train.columns if x not in target]\n",
    "result = modelfit(clf,train,predictors,useTrainCV=True, cv_folds=5, early_stopping_rounds=50)\n",
    "\n",
    "s_result = []\n",
    "s_result = s_result.append(result)\n",
    "s_result = pd.Series(result)\n",
    "s_result.to_csv(\"predict_result_xgb.csv\")\n",
    "\n",
    "\n",
    "clf.fit(X_train, y_train)\n",
    "\n",
    "test.shot_made_flag = [i[1] for i in clf.predict_proba(X_test)]\n",
    "\n",
    "test[['shot_id', 'shot_made_flag']].to_csv('sub.csv', index=False)\n",
    "predictions_train = clf.predict_proba(X_train)\n",
    "features_train = pd.DataFrame({'shot_made_flag': predictions_train[:, 1]})\n",
    "features_train[['shot_made_flag']].to_csv('features_train.csv', index=False)\n",
    "\n",
    "\n",
    "# In[ ]:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "param_test1 = [{\n",
    " 'max_depth':[9,11,13],\n",
    " 'min_child_weight':[1,3]\n",
    "}]\n",
    "gsearch1 = GridSearchCV(estimator = xgb.XGBClassifier(learning_rate =0.03, n_estimators=200,\n",
    " min_child_weight=1, gamma=0, subsample=0.8, colsample_bytree=0.8,\n",
    " objective= 'binary:logistic', nthread=4, scale_pos_weight=1, seed=2), \n",
    "                        param_grid = param_test1, scoring='log_loss',n_jobs=4,iid=False, cv=5)\n",
    "gsearch1.fit(train[predictors],train[target])\n",
    "gsearch1.grid_scores_, gsearch1.best_params_, gsearch1.best_score_\n",
    "\n",
    "gsresult = [gsearch1.grid_scores_, gsearch1.best_params_, gsearch1.best_score_]\n",
    "\n",
    "print gsresult\n",
    "gs_result = pd.Series(gsresult)\n",
    "gs_result.to_csv(\"gs_result_xgb.csv\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 2",
   "language": "python",
   "name": "python2"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
